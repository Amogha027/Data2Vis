{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = []\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            seq = json.loads(line)\n",
    "            data.append(seq)\n",
    "    return data\n",
    "\n",
    "def get_input_vocab(data):\n",
    "    char_vocab = set()\n",
    "    for seq in data:\n",
    "        for _, value in seq[0].items():\n",
    "            char_vocab.update(str(value))\n",
    "    return char_vocab\n",
    "\n",
    "def get_output_vocab(data):\n",
    "    char_vocab = set()\n",
    "    for obj in data:\n",
    "        char_vocab.update(json.dumps(obj))\n",
    "    return char_vocab\n",
    "\n",
    "def get_input_idx(data, ch2idx):\n",
    "    result = []\n",
    "    for seq in data:\n",
    "        indices = []\n",
    "        for _, value in seq[0].items():\n",
    "            indices.extend([ch2idx[char] for char in str(value)])\n",
    "        result.append(indices)\n",
    "    return result\n",
    "\n",
    "def get_output_idx(data, ch2idx):\n",
    "    result = []\n",
    "    for obj in data:\n",
    "        indices = [ch2idx[char] for char in json.dumps(obj)]\n",
    "        result.append(indices)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "train_path = ['A3 files/train.sources', 'A3 files/train.targets']\n",
    "val_path = ['A3 files/dev.sources', 'A3 files/dev.targets']\n",
    "test_path = ['A3 files/test.sources', 'A3 files/test.targets']\n",
    "\n",
    "X, Y = load_data(train_path[0]), load_data(train_path[1])\n",
    "x_valid, y_valid = load_data(val_path[0], val_path[1])\n",
    "x_test, y_test = load_data(test_path[0], test_path[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172719\n",
      "172719\n",
      "97\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "vocab_input = get_input_vocab(X)\n",
    "vocab_output = get_output_vocab(Y)\n",
    "\n",
    "special_tokens = ['<sos>', '<pad>', '<eos>']\n",
    "vocab_input.update(special_tokens)\n",
    "vocab_output.update(special_tokens)\n",
    "\n",
    "ch2idx_input = {char: idx for idx, char in enumerate(vocab_input)}\n",
    "idx2ch_input = {idx: char for char, idx in ch2idx_input.items()}\n",
    "\n",
    "ch2idx_output = {char: idx for idx, char in enumerate(vocab_output)}\n",
    "idx2ch_output = {idx: char for char, idx in ch2idx_output.items()}\n",
    "\n",
    "x_train = get_input_idx(X, ch2idx_input)\n",
    "y_train = get_output_idx(Y, ch2idx_output)\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "\n",
    "x_train = pad_sequence([torch.tensor(seq) for seq in x_train], batch_first=True, padding_value=ch2idx_input['<pad>'])\n",
    "y_train = pad_sequence([torch.tensor(seq) for seq in y_train], batch_first=True, padding_value=ch2idx_output['<pad>'])\n",
    "\n",
    "trainset = TensorDataset(x_train, y_train)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(len(ch2idx_input))\n",
    "print(len(ch2idx_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = get_input_idx(X, ch2idx_input)\n",
    "y_test = get_output_idx(Y, ch2idx_output)\n",
    "\n",
    "x_test = pad_sequence([torch.tensor(seq) for seq in x_test], batch_first=True, padding_value=ch2idx_input['<pad>'])\n",
    "y_test = pad_sequence([torch.tensor(seq) for seq in y_test], batch_first=True, padding_value=ch2idx_output['<pad>'])\n",
    "\n",
    "testset = TensorDataset(x_test, y_test)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the classes\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size, input_size, hidden_size, num_layers, rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, bidirectional=True)\n",
    "        self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc_cell = nn.Linear(hidden_size*2, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        en_states, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
    "        return en_states, hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, output_size, hidden_size, num_layers, rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        self.embedding = nn.Embedding(output_size, embed_size)\n",
    "        self.lstm = nn.LSTM(hidden_size*2 + embed_size, hidden_size, num_layers)\n",
    "        self.energy = nn.Linear(hidden_size*3, 1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, en_states, hidden, cell):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        sequence_length = en_states.shape[0]\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
    "        energy = self.relu(self.energy(torch.cat((h_reshaped, en_states), dim=2)))\n",
    "        attention = self.softmax(energy)\n",
    "        attention = attention.permute(1, 2, 0)\n",
    "        en_states = en_states.permute(1, 0, 2)\n",
    "        context_vector = torch.bmm(attention, en_states).permute(1, 0, 2)\n",
    "        de_input = torch.cat((context_vector, embedded), dim=2)\n",
    "        outputs, (hidden, cell) = self.lstm(de_input, (hidden, cell))\n",
    "        prediction = self.fc(outputs).squeeze(0)\n",
    "        return prediction, hidden, cell\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        tgt_vocab = self.decoder.output_size\n",
    "        outputs = torch.zeros(target_len, batch_size, tgt_vocab)\n",
    "        en_states, hidden, cell = self.encoder(source)\n",
    "        x = target[0]\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, en_states, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            x = output.argmax(1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0813\n",
      "Epoch 2/10, Loss: 0.0653\n",
      "Epoch 3/10, Loss: 0.0549\n",
      "Epoch 4/10, Loss: 0.0446\n",
      "Epoch 5/10, Loss: 0.0369\n",
      "Epoch 6/10, Loss: 0.0305\n",
      "Epoch 7/10, Loss: 0.0259\n",
      "Epoch 8/10, Loss: 0.0220\n",
      "Epoch 9/10, Loss: 0.0186\n",
      "Epoch 10/10, Loss: 0.0159\n"
     ]
    }
   ],
   "source": [
    "input_vocab_size = len(ch2idx_input) + 1\n",
    "output_vocab_size = len(ch2idx_output) + 1\n",
    "hidden_size = 512\n",
    "embed_size = 512\n",
    "num_layers = 1\n",
    "dropout_rate = 0.5\n",
    "learn_rate = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "encoder_net = Encoder(embed_size, input_vocab_size, hidden_size, num_layers, dropout_rate)\n",
    "decoder_net = Decoder(embed_size, output_vocab_size, hidden_size, num_layers, dropout_rate)\n",
    "\n",
    "model = Seq2Seq(encoder_net, decoder_net)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=ch2idx_output['<eos>'])\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    t_loss = 0\n",
    "    for (input, target) in trainloader:\n",
    "        input = input.permute(1, 0)\n",
    "        target = target.permute(1, 0)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input, target)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        loss = loss_func(output, target)\n",
    "        t_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Steps: {}/{} Loss: {:.4f}'.format(1+_, num_epochs, t_loss/len(trainloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_perplexity(model, dataloader):\n",
    "    log_ppx = 0\n",
    "    for (input, target) in dataloader:\n",
    "        input = input.permute(1, 0)\n",
    "        target = target.permute(1, 0)\n",
    "        output = model(input, target)\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target.reshape(-1)\n",
    "        log_ppx += loss_func(output, target).item()\n",
    "    return log_ppx / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.073368463047935463\n"
     ]
    }
   ],
   "source": [
    "log_perplexity(model, testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
